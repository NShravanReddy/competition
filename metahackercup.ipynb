{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "name": "metahackercup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NShravanReddy/competition/blob/main/metahackercup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n",
        "\n",
        "Features in the notebook:\n",
        "1. Uses Maxime Labonne's [FineTome 100K](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset.\n",
        "1. Convert ShareGPT to HuggingFace format via `standardize_sharegpt`\n",
        "2. Train on Completions / Assistant only via `train_on_responses_only`\n",
        "3. Unsloth now supports Torch 2.4, all TRL & Xformers versions & Python 3.12!"
      ],
      "metadata": {
        "id": "IqM-T1RTzY6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "2eSvM9zX_2d3",
        "execution": {
          "iopub.status.busy": "2024-10-16T13:51:39.72268Z",
          "iopub.execute_input": "2024-10-16T13:51:39.723204Z",
          "iopub.status.idle": "2024-10-16T13:54:55.265129Z",
          "shell.execute_reply.started": "2024-10-16T13:51:39.723172Z",
          "shell.execute_reply": "2024-10-16T13:54:55.263896Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ],
      "metadata": {
        "id": "r2v_X2fA0Df5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # Can select any from the below:\n",
        "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
        "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
        "    # And also all Instruct versions and Math. Coding verisons!\n",
        "    model_name = \"unsloth/Qwen2.5-7B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "execution": {
          "iopub.status.busy": "2024-10-16T13:54:55.267732Z",
          "iopub.execute_input": "2024-10-16T13:54:55.268203Z",
          "iopub.status.idle": "2024-10-16T13:55:41.656297Z",
          "shell.execute_reply.started": "2024-10-16T13:54:55.268132Z",
          "shell.execute_reply": "2024-10-16T13:55:41.655247Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "SXd9bTZd1aaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "execution": {
          "iopub.status.busy": "2024-10-16T13:58:29.611713Z",
          "iopub.execute_input": "2024-10-16T13:58:29.612134Z",
          "iopub.status.idle": "2024-10-16T13:58:34.850288Z",
          "shell.execute_reply.started": "2024-10-16T13:58:29.612096Z",
          "shell.execute_reply": "2024-10-16T13:58:34.849279Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Qwen-2.5` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Qwen2.5 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|im_start|>system\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "Hello!<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Hey there! How are you?<|im_end|>\n",
        "<|im_start|>user\n",
        "I'm great thanks!<|im_end|>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
      ],
      "metadata": {
        "id": "vITh0KVJ10qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Explore the dataset structure without loading the entire split\n",
        "dataset_info = load_dataset(\"hackercupai/hackercup\")\n",
        "\n",
        "# Print the dataset information to see available splits\n",
        "print(dataset_info)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T13:58:39.793347Z",
          "iopub.execute_input": "2024-10-16T13:58:39.794183Z",
          "iopub.status.idle": "2024-10-16T13:59:23.78264Z",
          "shell.execute_reply.started": "2024-10-16T13:58:39.794142Z",
          "shell.execute_reply": "2024-10-16T13:59:23.781408Z"
        },
        "trusted": true,
        "id": "VrfqsSnu3gqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "# Load the dataset (train split)\n",
        "dataset = load_dataset(\"hackercupai/hackercup\", split=\"sample\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T13:59:23.784819Z",
          "iopub.execute_input": "2024-10-16T13:59:23.78532Z",
          "iopub.status.idle": "2024-10-16T13:59:24.601307Z",
          "shell.execute_reply.started": "2024-10-16T13:59:23.785274Z",
          "shell.execute_reply": "2024-10-16T13:59:24.60054Z"
        },
        "trusted": true,
        "id": "NjVOSHLX3gqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"qwen-2.5\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"hackercupai/hackercup\", split = \"sample\")"
      ],
      "metadata": {
        "id": "LjY75GoYUCB8",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:10:06.840346Z",
          "iopub.execute_input": "2024-10-16T14:10:06.840817Z",
          "iopub.status.idle": "2024-10-16T14:10:07.756485Z",
          "shell.execute_reply.started": "2024-10-16T14:10:06.840777Z",
          "shell.execute_reply": "2024-10-16T14:10:07.755747Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```"
      ],
      "metadata": {
        "id": "K9CBpiISFa6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"hackercupai/hackercup\", split=\"sample\")\n",
        "\n",
        "# Define the formatting prompts function\n",
        "def formatting_prompts_func(examples):\n",
        "    return examples\n",
        "#dataset = standardize_sharegpt(dataset)\n",
        "# Apply the formatting prompts function to the dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Print the updated dataset\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T14:23:32.226526Z",
          "iopub.execute_input": "2024-10-16T14:23:32.227149Z",
          "iopub.status.idle": "2024-10-16T14:23:32.693773Z",
          "shell.execute_reply.started": "2024-10-16T14:23:32.227112Z",
          "shell.execute_reply": "2024-10-16T14:23:32.692916Z"
        },
        "trusted": true,
        "id": "4YEnZ7NU3gqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from unsloth.chat_templates import standardize_sharegpt\n",
        "#dataset = standardize_sharegpt(dataset)\n",
        "#dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "oPXzJZzHEgXe",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:17:33.60822Z",
          "iopub.execute_input": "2024-10-16T14:17:33.609042Z",
          "iopub.status.idle": "2024-10-16T14:17:33.612693Z",
          "shell.execute_reply.started": "2024-10-16T14:17:33.609006Z",
          "shell.execute_reply": "2024-10-16T14:17:33.61162Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We look at how the conversations are structured for item 5:"
      ],
      "metadata": {
        "id": "ndDUB23CGAC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[5]['year']"
      ],
      "metadata": {
        "id": "gGFzmplrEy9I",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:24:03.47087Z",
          "iopub.execute_input": "2024-10-16T14:24:03.471851Z",
          "iopub.status.idle": "2024-10-16T14:24:03.478112Z",
          "shell.execute_reply.started": "2024-10-16T14:24:03.471809Z",
          "shell.execute_reply": "2024-10-16T14:24:03.477159Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we see how the chat template transformed these conversations.\n",
        "\n",
        "**[Notice]** Qwen 2.5 Instruct's default chat template default adds `\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"`, so do not be alarmed!"
      ],
      "metadata": {
        "id": "GfzTdMtvGE6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[5][\"statement\"]"
      ],
      "metadata": {
        "id": "vhXv0xFMGNKE",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:24:23.879841Z",
          "iopub.execute_input": "2024-10-16T14:24:23.880422Z",
          "iopub.status.idle": "2024-10-16T14:24:23.886869Z",
          "shell.execute_reply.started": "2024-10-16T14:24:23.880384Z",
          "shell.execute_reply": "2024-10-16T14:24:23.885911Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ],
      "metadata": {
        "id": "idAEIeSQ3xdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"statement\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.I modified this line is comment\n",
        "        max_steps = 10,#check this value\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "95_Nn-89DhsL",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:51:07.570619Z",
          "iopub.execute_input": "2024-10-16T14:51:07.570991Z",
          "iopub.status.idle": "2024-10-16T14:51:07.853536Z",
          "shell.execute_reply.started": "2024-10-16T14:51:07.570958Z",
          "shell.execute_reply": "2024-10-16T14:51:07.852712Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ],
      "metadata": {
        "id": "C_sGp5XlG6dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|im_start|>user\\n\",\n",
        "    response_part = \"<|im_start|>assistant\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "juQiExuBG5Bt",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:51:10.470564Z",
          "iopub.execute_input": "2024-10-16T14:51:10.47134Z",
          "iopub.status.idle": "2024-10-16T14:51:10.486981Z",
          "shell.execute_reply.started": "2024-10-16T14:51:10.471301Z",
          "shell.execute_reply": "2024-10-16T14:51:10.48623Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We verify masking is actually done:"
      ],
      "metadata": {
        "id": "Dv1NBUozV78l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ],
      "metadata": {
        "id": "LtsMVtlkUhja",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:51:12.748309Z",
          "iopub.execute_input": "2024-10-16T14:51:12.74923Z",
          "iopub.status.idle": "2024-10-16T14:51:12.765347Z",
          "shell.execute_reply.started": "2024-10-16T14:51:12.749186Z",
          "shell.execute_reply": "2024-10-16T14:51:12.764442Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == 0 else x for x in trainer.train_dataset[5][\"input_ids\"]])"
      ],
      "metadata": {
        "id": "_rD6fl8EUxnG",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:51:14.648723Z",
          "iopub.execute_input": "2024-10-16T14:51:14.64909Z",
          "iopub.status.idle": "2024-10-16T14:51:14.66539Z",
          "shell.execute_reply.started": "2024-10-16T14:51:14.649058Z",
          "shell.execute_reply": "2024-10-16T14:51:14.664548Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the System and Instruction prompts are successfully masked!"
      ],
      "metadata": {
        "id": "3enWUM0jV-jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:51:17.060558Z",
          "iopub.execute_input": "2024-10-16T14:51:17.060986Z",
          "iopub.status.idle": "2024-10-16T14:51:17.069051Z",
          "shell.execute_reply.started": "2024-10-16T14:51:17.060938Z",
          "shell.execute_reply": "2024-10-16T14:51:17.068151Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:51:19.200082Z",
          "iopub.execute_input": "2024-10-16T14:51:19.20115Z",
          "iopub.status.idle": "2024-10-16T14:51:25.910881Z",
          "shell.execute_reply.started": "2024-10-16T14:51:19.201093Z",
          "shell.execute_reply": "2024-10-16T14:51:25.909346Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:41:39.974134Z",
          "iopub.execute_input": "2024-10-16T14:41:39.974449Z",
          "iopub.status.idle": "2024-10-16T14:41:39.984599Z",
          "shell.execute_reply.started": "2024-10-16T14:41:39.974416Z",
          "shell.execute_reply": "2024-10-16T14:41:39.983684Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ],
      "metadata": {
        "id": "ekOmTR1hSNcr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kR3gIAX-SM2q",
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-10-16T14:41:39.986694Z",
          "iopub.execute_input": "2024-10-16T14:41:39.987058Z",
          "iopub.status.idle": "2024-10-16T14:43:02.651661Z",
          "shell.execute_reply.started": "2024-10-16T14:41:39.987016Z",
          "shell.execute_reply": "2024-10-16T14:43:02.650746Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ],
      "metadata": {
        "id": "CrSvZObor0lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\" Give me code in c language.\n",
        "    As the queen of an ant colony, it‚Äôs your job to ensure that the *ant*ire colony works together. Your colony has \\(N\\) worker ants, the \\(i\\)th of which is curr*ant*ly at coordinates \\((X_i, Y_i)\\). To align the efforts of all of your worker ants, you would like them to all be on the same line on the plane. How many of your ants need to move to get them to all lie on the same line?\n",
        "\n",
        "As is frequ*ant*ly the case in managem*ant*, you don‚Äôt need an exact answer, but you do need some degree of accuracy. If the true minimum number of ants that need to move is \\(M\\), then any answer between \\(M\\) and \\(2*M\\) (inclusive) will be accepted.\n",
        "\n",
        "# Constraints\n",
        "\\(1 \\leq T \\leq 75\\)\n",
        "\\(2 \\leq N \\leq 1{,}000{,}000\\)\n",
        "\\(0 \\leq |X_i|, |Y_i| \\leq 1{,}000{,}000{,}000\\)\n",
        "\n",
        "In each test case, no two ants will be at the same position.\n",
        "\n",
        "The sum of \\(N\\) across all test cases is at most \\(4{,}000{,}000\\).\n",
        "\n",
        "# Input Format\n",
        "Input begins with an integer \\(T\\), the number of test cases. Each case starts with a line that contains the integer \\(N\\). Then \\(N\\) lines follow, the \\(i\\)th of which contains the integers \\(X_i\\) and \\(Y_i\\).\n",
        "3\n",
        "7\n",
        "4 8\n",
        "2 4\n",
        "7 2\n",
        "6 10\n",
        "0 1\n",
        "3 4\n",
        "4 7\n",
        "4\n",
        "1 1\n",
        "-1 1\n",
        "1 -1\n",
        "-1 -1\n",
        "4\n",
        "1 1\n",
        "2 2\n",
        "-3 -3\n",
        "4 4\n",
        "\n",
        "# Output Format\n",
        "For the \\(i\\)th test case, print \"`Case #i:` \" followed by the number of ants you need to move to get all of the ants to lie on the same line.\n",
        "Case #1: 3\n",
        "Case #2: 2\n",
        "Case #3: 0\n",
        "\n",
        "# Sample Explanation\n",
        "In the first case, the \\(4\\) ants are all on the line \\(y = x\\), so no ants need to be moved. \\(0\\) is the only answer that will be accepted for this case.\n",
        "\n",
        "In the second case, the \\(4\\) ants are at the vertices of a square, so every line contains at most \\(2\\) of the \\(4\\) ants. \\(2\\) ants need to be moved, so the answers \\(2\\), \\(3\\), and \\(4\\) will be accepted for this case.\n",
        "\n",
        "The third case is depicted below. Ants \\(2\\), \\(4\\), \\(5\\), and \\(7\\) all lie on the line \\(y = \\frac{3}{2}x + 1\\). Moving the other \\(3\\) ants is the optimal way to get all of the ants on a single line, so any answer between \\(3\\) and \\(6\\) inclusive will be accepted for this case.\n",
        "\n",
        "{{PHOTO_ID:1528828545185163|WIDTH:400}}\n",
        "    \"\"\"\n",
        "     },\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1028,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "id": "e2pEuRb1r2Vg",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:44:01.744797Z",
          "iopub.execute_input": "2024-10-16T14:44:01.745161Z",
          "iopub.status.idle": "2024-10-16T14:44:50.832941Z",
          "shell.execute_reply.started": "2024-10-16T14:44:01.745128Z",
          "shell.execute_reply": "2024-10-16T14:44:50.832181Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ],
      "metadata": {
        "id": "uMuVrWbjAzhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "model.push_to_hub(\"Nadiveedishravanreddy/lora_model\", token = \"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\") # Online saving\n",
        "tokenizer.push_to_hub(\"Nadiveedishravanreddy/lora_model\", token = \"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\") # Online saving"
      ],
      "metadata": {
        "id": "upcOlWe7A1vc",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:52:01.434043Z",
          "iopub.execute_input": "2024-10-16T14:52:01.434457Z",
          "iopub.status.idle": "2024-10-16T14:52:04.228863Z",
          "shell.execute_reply.started": "2024-10-16T14:52:01.434417Z",
          "shell.execute_reply": "2024-10-16T14:52:04.228133Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ],
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"qwen-2.5\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\":\n",
        "     \"\"\" Give me code in c language.\n",
        "    As the queen of an ant colony, it‚Äôs your job to ensure that the *ant*ire colony works together. Your colony has \\(N\\) worker ants, the \\(i\\)th of which is curr*ant*ly at coordinates \\((X_i, Y_i)\\). To align the efforts of all of your worker ants, you would like them to all be on the same line on the plane. How many of your ants need to move to get them to all lie on the same line?\n",
        "\n",
        "As is frequ*ant*ly the case in managem*ant*, you don‚Äôt need an exact answer, but you do need some degree of accuracy. If the true minimum number of ants that need to move is \\(M\\), then any answer between \\(M\\) and \\(2*M\\) (inclusive) will be accepted.\n",
        "\n",
        "# Constraints\n",
        "\\(1 \\leq T \\leq 75\\)\n",
        "\\(2 \\leq N \\leq 1{,}000{,}000\\)\n",
        "\\(0 \\leq |X_i|, |Y_i| \\leq 1{,}000{,}000{,}000\\)\n",
        "\n",
        "In each test case, no two ants will be at the same position.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Output Format\n",
        "For the \\(i\\)th test case, print \"`Case #i:` \" followed by the number of ants you need to move to get all of the ants to lie on the same line.\n",
        "Case #1: 3\n",
        "Case #2: 2\n",
        "Case #3: 0\n",
        "\n",
        "# Sample Explanation\n",
        "In the first case, the \\(4\\) ants are all on the line \\(y = x\\), so no ants need to be moved. \\(0\\) is the only answer that will be accepted for this case.\n",
        "\n",
        "In the second case, the \\(4\\) ants are at the vertices of a square, so every line contains at most \\(2\\) of the \\(4\\) ants. \\(2\\) ants need to be moved, so the answers \\(2\\), \\(3\\), and \\(4\\) will be accepted for this case.\n",
        "\n",
        "The third case is depicted below. Ants \\(2\\), \\(4\\), \\(5\\), and \\(7\\) all lie on the line \\(y = \\frac{3}{2}x + 1\\). Moving the other \\(3\\) ants is the optimal way to get all of the ants on a single line, so any answer between \\(3\\) and \\(6\\) inclusive will be accepted for this case.\n",
        "\n",
        "{{PHOTO_ID:1528828545185163|WIDTH:400}}\n",
        "    \"\"\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "MKX_XKs_BNZR",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:52:19.695803Z",
          "iopub.execute_input": "2024-10-16T14:52:19.696208Z",
          "iopub.status.idle": "2024-10-16T14:52:21.744643Z",
          "shell.execute_reply.started": "2024-10-16T14:52:19.696164Z",
          "shell.execute_reply": "2024-10-16T14:52:21.742952Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ],
      "metadata": {
        "id": "QQMjaNrjsU5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "yFfaXG0WsQuE",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:52:36.61648Z",
          "iopub.execute_input": "2024-10-16T14:52:36.61689Z",
          "iopub.status.idle": "2024-10-16T14:52:36.623371Z",
          "shell.execute_reply.started": "2024-10-16T14:52:36.616851Z",
          "shell.execute_reply": "2024-10-16T14:52:36.622537Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ],
      "metadata": {
        "id": "f422JgM9sdVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ],
      "metadata": {
        "id": "iHjt_SMYsd3P",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:52:40.43725Z",
          "iopub.execute_input": "2024-10-16T14:52:40.437955Z",
          "iopub.status.idle": "2024-10-16T14:52:40.444824Z",
          "shell.execute_reply.started": "2024-10-16T14:52:40.437912Z",
          "shell.execute_reply": "2024-10-16T14:52:40.443945Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ],
      "metadata": {
        "id": "TCv4vXHd61i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ],
      "metadata": {
        "id": "FqfebeAdT073",
        "execution": {
          "iopub.status.busy": "2024-10-16T14:59:05.422116Z",
          "iopub.execute_input": "2024-10-16T14:59:05.422582Z",
          "iopub.status.idle": "2024-10-16T14:59:05.431065Z",
          "shell.execute_reply.started": "2024-10-16T14:59:05.422541Z",
          "shell.execute_reply": "2024-10-16T14:59:05.430031Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ],
      "metadata": {
        "id": "bDp0zNpwe6U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ],
      "metadata": {
        "id": "Zt9CHJqO6p30"
      }
    }
  ]
}